RESULTS FOR : epoch = 1, batch = 128, lr = 0.001
test-acc5
[0.24111675126903553]
------------------------------
train-acc
[0.043586550435865505]
------------------------------
test-loss
[2.9957203961870995]
------------------------------
train-acc5
[0.23972602739726026]
------------------------------
train-loss
[2.995740698699191]
------------------------------
test-acc
[0.0532994923857868]
------------------------------

RESULTS FOR : epoch = 3, batch = 128, lr = 0.001
test-loss
[2.9957380863615706, 2.9957380076955418, 2.995737904824581]
------------------------------
train-loss
[2.995743135141109, 2.9957305788251563, 2.995720013586403]
------------------------------
test-acc5
[0.22842639593908629, 0.22842639593908629, 0.22842639593908629]
------------------------------
test-acc
[0.050761421319796954, 0.050761421319796954, 0.050761421319796954]
------------------------------
train-acc5
[0.23785803237858033, 0.2465753424657534, 0.25840597758405975]
------------------------------
train-acc
[0.051681195516811954, 0.04919053549190536, 0.05105853051058531]
------------------------------


RESULTS FOR : epoch = 5, batch = 128, lr = 0.001
train-acc
[0.047322540473225407, 0.04919053549190536, 0.048567870485678705, 0.052303860523038606, 0.058530510585305104]
------------------------------
train-acc5
[0.23848069738480698, 0.2503113325031133, 0.2640099626400996, 0.26899128268991285, 0.2546699875466999]
------------------------------
test-acc
[0.0532994923857868, 0.0532994923857868, 0.0532994923857868, 0.0532994923857868, 0.0532994923857868]
------------------------------
train-loss
[2.9957442642891245, 2.995736090065088, 2.9957314389728817, 2.995733090682166, 2.995734110270461]
------------------------------
test-acc5
[0.24873096446700507, 0.2512690355329949, 0.2512690355329949, 0.2512690355329949, 0.2512690355329949]
------------------------------
test-loss
[2.995725222650518, 2.9957249830216925, 2.9957256220318946, 2.995725044744269, 2.9957254162899734]
------------------------------

RESULTS FOR : epoch = 10, batch = 128, lr = 0.001
test-acc
[0.050761421319796954, 0.050761421319796954, 0.050761421319796954, 0.050761421319796954, 0.050761421319796954, 0.048223350253807105, 0.050761421319796954, 0.050761421319796954, 0.050761421319796954, 0.050761421319796954]
------------------------------
train-acc5
[0.24906600249066002, 0.24782067247820672, 0.24719800747198006, 0.2627646326276463, 0.2503113325031133, 0.2521793275217933, 0.24283935242839352, 0.2403486924034869, 0.25716064757160645, 0.26089663760896636]
------------------------------
test-loss
[2.9957365263537103, 2.9957373977312582, 2.9957369547810044, 2.995736861592017, 2.9957371520511997, 2.9957370891183768, 2.9957371193745415, 2.995737185938104, 2.9957372658143795, 2.9957376651957555]
------------------------------
train-acc
[0.053549190535491904, 0.05292652552926526, 0.06351183063511831, 0.052303860523038606, 0.053549190535491904, 0.055417185554171855, 0.05105853051058531, 0.0547945205479452, 0.05105853051058531, 0.052303860523038606]
------------------------------
test-acc5
[0.2512690355329949, 0.2512690355329949, 0.2512690355329949, 0.2512690355329949, 0.2512690355329949, 0.2512690355329949, 0.2512690355329949, 0.2512690355329949, 0.2512690355329949, 0.2512690355329949]
------------------------------
train-loss
[2.995725854990046, 2.9957253374762436, 2.9957211014639546, 2.9957259298113184, 2.9957292044296358, 2.995723526920209, 2.9957321580884466, 2.9957280631083183, 2.995721010609552, 2.9957293398205103]
------------------------------
